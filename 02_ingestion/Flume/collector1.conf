# Define an Avro source:
collector1.sources=r1
collector1.sources.r1.type=avro
collector1.sources.r1.bind=0.0.0.0
collector1.sources.r1.port=4141
collector1.sources.r1.channels=ch1
# Define a file channel using multiple disks for reliability:
collector1.channels=ch1
collector1.channels.ch1.type=FILE
collector1.channels.ch1.checkpointDir=/opt/flume/ch1/checkpoint
collector1.channels.ch1.dataDirs=/opt/flume/ch1/data1,/opt/flume/ch1/data2
# Define HDFS sinks to persist events as text. 
# Note the use of multiple sinks to spread the load:
collector1.sinks=k1 k2
collector1.sinks.k1.type=hdfs
collector1.sinks.k1.channel=ch1
# Partition files by date:
#collector1.sinks.k1.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=%Y/month=%m/day=%d
collector1.sinks.k1.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=2014/month=10/day=10
collector1.sinks.k1.hdfs.filePrefix=combined
collector1.sinks.k1.hdfs.fileSuffix=.log
collector1.sinks.k1.hdfs.fileType=DataStream
collector1.sinks.k1.hdfs.writeFormat=Text
collector1.sinks.k1.hdfs.batchSize=1000 
# Roll HDFS files every 10000 events or 30 seconds:
collector1.sinks.k1.hdfs.rollInterval=30
collector1.sinks.k1.hdfs.rollCount=10000
collector1.sinks.k1.hdfs.rollSize=0
collector1.sinks.k2.type=hdfs
collector1.sinks.k2.channel=ch1
# Partition files by date:
#collector1.sinks.k2.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=%Y/month=%m/day=%d
collector1.sinks.k2.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=2014/month=10/day=10
collector1.sinks.k2.hdfs.filePrefix=combined
collector1.sinks.k2.hdfs.fileSuffix=.log
collector1.sinks.k2.hdfs.fileType=DataStream
collector1.sinks.k2.hdfs.writeFormat=Text
collector1.sinks.k2.hdfs.batchSize=1000 
collector1.sinks.k2.hdfs.rollInterval=30
collector1.sinks.k2.hdfs.rollCount=10000
collector1.sinks.k2.hdfs.rollSize=0
collector1.sinkgroups=g1
collector1.sinkgroups.g1.sinks=k1 k2
collector1.sinkgroups.g1.processor.type=load_balance
collector1.sinkgroups.g1.processor.selector=round_robin
collector1.sinkgroups.g1.processor.backoff=true

